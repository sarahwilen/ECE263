# -*- coding: utf-8 -*-
"""CS263_Project_Accuracy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k5dxhxwTdlEfEY8f4KfUihmIScmHSQ6y

# **BERT Score**
Ref: https://arxiv.org/abs/1904.09675v3
"""

# Reference:
# https://haticeozbolat17.medium.com/text-summarization-how-to-calculate-bertscore-771a51022964#:~:text=BertScore%20is%20a%20method%20used,gram%2Dbased%20metrics%20often%20encounter

from transformers import BertTokenizer, BertModel
import torch
import numpy as np

# Load the pre-trained BERT model and tokenizer
bert_tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
bert_model = BertModel.from_pretrained("bert-base-uncased")

def get_BERT_score(text1, text2):
  # Prepare the texts for BERT
  inputs1 = bert_tokenizer(text1, return_tensors="pt", padding=True, truncation=True)
  inputs2 = bert_tokenizer(text2, return_tensors="pt", padding=True, truncation=True)

  # Feed the texts to the BERT model
  outputs1 = bert_model(**inputs1)
  outputs2 = bert_model(**inputs2)

  # Obtain the representation vectors
  embeddings1 = outputs1.last_hidden_state.mean(dim=1).detach().numpy()
  embeddings2 = outputs2.last_hidden_state.mean(dim=1).detach().numpy()

  # Calculate cosine similarity
  similarity = np.dot(embeddings1, embeddings2.T) / (np.linalg.norm(embeddings1) * np.linalg.norm(embeddings2))
  return similarity[0][0]
  # print("Similarity between the texts: {:.4f}".format(similarity[0][0]))

# Commented out IPython magic to ensure Python compatibility.
## Call Chat GPT:
from google.colab import drive
drive.mount('/content/drive')
# %cd /content/drive/My Drive
!pip install openai
from openai import OpenAI
import re



orgnizationID = 'org-QKcHw0yYSRTm4BnQPjG9ot6F'
def ask_chatgpt(prompt):
    client = OpenAI(
      organization='', #REMOVED API KEY FOR PRIVACY
      api_key = '' # REMOVED API KEY FOR PRIVACY
    )
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
    )
    print(response)
    return response

def extract_response(chatgpt_output):

    extracted_events = []


    choices = chatgpt_output.choices
    if choices and len(choices) > 0:
        # Access the first choice
        choice = choices[0]
        message_content = choice.message.content
        return message_content
    return extracted_events

def extract_response_score(chatgpt_output):
    message_content = extract_response(chatgpt_output)

    scores = {
        "pairwise_comparison_score_chatgpt": None,
        "single_answer_grading_score_chatgpt": None,
        "reference_guided_grading_score_chatgpt": None,
        "pairwise_comparison_score_claude": None,
        "single_answer_grading_score_claude": None,
        "reference_guided_grading_score_claude": None,
    }

    score_patterns = {
        "pairwise_comparison_score_chatgpt": re.compile(r"Pairwise comparison score for ChatGPT[:\s]*([\d.]+)"),
        "single_answer_grading_score_chatgpt": re.compile(r"Single answer grading score for ChatGPT[:\s]*([\d.]+)"),
        "reference_guided_grading_score_chatgpt": re.compile(r"Reference-guided grading score for ChatGPT[:\s]*([\d.]+)"),
        "pairwise_comparison_score_claude": re.compile(r"Pairwise comparison score for Claude[:\s]*([\d.]+)"),
        "single_answer_grading_score_claude": re.compile(r"Single answer grading score for Claude[:\s]*([\d.]+)"),
        "reference_guided_grading_score_claude": re.compile(r"Reference-guided grading score for Claude[:\s]*([\d.]+)"),
    }

    for key, pattern in score_patterns.items():
        match = pattern.search(message_content)
        if match:
            scores[key] = float(match.group(1))

    return scores

"""# **BLEURT Score**
Ref: https://arxiv.org/abs/2004.04696
"""

!pip install evaluate
!pip install git+https://github.com/google-research/bleurt.git

!pip install tensorflow[and-cuda]
!pip install tensorrt

!wget https://storage.googleapis.com/bleurt-oss-21/BLEURT-20.zip .
!unzip "BLEURT-20.zip"
!python -m bleurt.score_files \
  -candidate_file=bleurt/test_data/candidates \
  -reference_file=bleurt/test_data/references \
  -bleurt_checkpoint=BLEURT-20

# Reference:
# https://github.com/google-research/bleurt/blob/master/README.md

from bleurt import score

def get_BLEURT_score(reference, candidate):
  checkpoint = "BLEURT-20"
  scorer = score.BleurtScorer(checkpoint)
  scores = scorer.score(references=[reference], candidates=[candidate])
  assert isinstance(scores, list) and len(scores) == 1
  return scores[0]

"""# **Flesch Reading Ease Score**
Ref: https://pypi.org/project/py-readability-metrics/


"""

!pip install py-readability-metrics
!python -m nltk.downloader punkt

from readability import Readability
string = 'Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, “and what is the use of a book,” thought Alice, “without pictures or conversations." Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, “and what is the use of a book,” thought Alice, “without pictures or conversations."'
string2 = 'Dick and Jane run. Dick and Jane are fast. They fell down a hill. Dick and Jane run. Dick and Jane are fast. They fell down a hill. Dick and Jane run. Dick and Jane are fast. They fell down a hill. Dick and Jane run. Dick and Jane are fast. They fell down a hill. Dick and Jane run. Dick and Jane are fast. They fell down a hill. Dick and Jane run. Dick and Jane are fast. They fell down a hill. Dick and Jane run. Dick and Jane are fast. They fell down a hill. Dick and Jane run. Dick and Jane are fast. They fell down a hill. Dick and Jane run. Dick and Jane are fast. They fell down a hill. Dick and Jane run. Dick and Jane are fast. They fell down a hill.'

# print(string.count(' '))
array =[string, string2]
r = Readability(array[0])
f = r.flesch()
print(f.score)
print(f.ease)
print(f.grade_levels)
k = r.flesch_kincaid()
print(k.score)

!pip install textstat
import textstat as ts

string = "Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, 'and what is the use of a book,' thought Alice 'without pictures or conversation?' So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, when suddenly a White Rabbit with pink eyes ran close by her."
string2 = 'Dick and Jane run. Dick and Jane are fast. They fell down a hill. Dick and Jane run. Dick and Jane are fast. They fell down a hill. Dick and Jane run. Dick and Jane are fast. They fell down a hill. Dick and Jane run. Dick and Jane are fast. They fell down a hill. Dick and Jane run. Dick and Jane are fast. They fell down a hill. Dick and Jane run. Dick and Jane are fast. They fell down a hill. Dick and Jane run. Dick and Jane are fast. They fell down a hill. Dick and Jane run. Dick and Jane are fast. They fell down a hill. Dick and Jane run. Dick and Jane are fast. They fell down a hill. Dick and Jane run. Dick and Jane are fast. They fell down a hill.'
string3 = "You are cool."
string4 = "All children, except one, grow up. They soon know that they will grow up, and the way Wendy knew was this. One day when she was two years old she was playing in a garden, and she plucked another flower and ran with it to her mother. I suppose she must have looked rather delightful, for Mrs. Darling put her hand to her heart and cried, 'Oh, why can't you remain like this for ever!' This was all that passed between them on the subject, but henceforth Wendy knew that she must grow up. You always know after you are two. Two is the beginning of the end."
ts.set_lang('en')
print(ts.flesch_kincaid_grade(string))
print(ts.flesch_reading_ease(string))
print(ts.flesch_kincaid_grade(string2))
print(ts.flesch_reading_ease(string2))
print(ts.flesch_kincaid_grade(string3))
print(ts.flesch_reading_ease(string3))
print(ts.flesch_kincaid_grade(string4))
print(ts.flesch_reading_ease(string4))

from google.colab import drive
drive.mount("/content/drive")

import pandas as pd

df_google_sheet = pd.read_csv('https://docs.google.com/spreadsheets/d/1Ya2q9Ao1J4nQoifs5pWVoDyjZ4G970bAiFCW0D87nYA/export?format=csv', usecols=['Question', 'LLM #1: ChatGPT Generation','LLM #2: Claude Generation'])

questions, gpt, claude = [], [], []
for index, row in df_google_sheet.iterrows():
  if index > 66:
    break
  if len(row['Question']) == 0:
      break
  questions.append(row['Question'])
  # print(row['Question'][:30])
  chat = row['LLM #1: ChatGPT Generation']
  cla = row['LLM #2: Claude Generation']
  if type(chat) != str or chat.count(' ') < 100:
    gpt.append("")
  else:
    r = Readability(chat)
    f = r.flesch()
    score = f.score
    gpt.append(score)
    # print(score)
  if type(cla) != str or cla.count(' ') < 100:
    claude.append("")
  else:
    r = Readability(cla)
    f = r.flesch()
    score = f.score
    claude.append(score)
    # print(score)

# return questions, gpt, claude
df_score = pd.DataFrame({'Questions': questions, 'GPT': gpt, 'Claude': claude})
display(df_score)

questions, gpt, claude = [], [], []
for index, row in df_google_sheet.iterrows():
  if index > 66:
    break
  if len(row['Question']) == 0:
      break
  questions.append(row['Question'])
  # print(row['Question'][:30])
  chat = row['LLM #1: ChatGPT Generation']
  cla = row['LLM #2: Claude Generation']
  if type(chat) != str or chat.count(' ') == 0:
    gpt.append("")
  else:
    score = ts.flesch_kincaid_grade(chat)
    gpt.append(score)
    print(score)
    print(ts.automated_readability_index(chat))
  if type(cla) != str or cla.count(' ') == 0:
    claude.append("")
  else:
    score = ts.flesch_kincaid_grade(cla)
    claude.append(score)
    print(score)
    print(ts.automated_readability_index(cla))

# return questions, gpt, claude
df_score_ts = pd.DataFrame({'Questions': questions, 'GPT': gpt, 'Claude': claude})
display(df_score_ts)

df_score_ts.to_csv('/content/drive/My Drive/Colab Notebooks/CS263/Project/ScoresFleschKincaidTextStat.csv', encoding='utf-8')
df_score.to_csv('/content/drive/My Drive/Colab Notebooks/CS263/Project/ScoresFleschReadability.csv', encoding='utf-8')



"""#**Topical Coherence**
Ref: https://github.com/raaga500/YTshared/blob/master/V4_TopicModelling_4.ipynb
Ref: https://radimrehurek.com/gensim_3.8.3/models/coherencemodel.html
"""

#Dependencies
import pandas as pd
import gensim #the library for Topic modelling
from gensim.models.ldamulticore import LdaMulticore
from gensim import corpora, models

import nltk
from nltk.corpus import stopwords
import string
from nltk.stem.wordnet import WordNetLemmatizer

import warnings
warnings.simplefilter('ignore')
from itertools import chain

df_topics = pd.read_csv('https://docs.google.com/spreadsheets/d/1Ya2q9Ao1J4nQoifs5pWVoDyjZ4G970bAiFCW0D87nYA/export?format=csv', usecols=['Practice Area','Question'])
df_topics = df_topics[:67]

print(df_topics)

stopwords = ["0o", "0s", "3a", "3b", "3d", "6b", "6o", "a", "a1", "a2", "a3", "a4", "ab", "able", "about", "above", "abst", "ac", "accordance", "according", "accordingly", "across", "act", "actually", "ad", "added", "adj", "ae", "af", "affected", "affecting", "affects", "after", "afterwards", "ag", "again", "against", "ah", "ain", "ain't", "aj", "al", "all", "allow", "allows", "almost", "alone", "along", "already", "also", "although", "always", "am", "among", "amongst", "amoungst", "amount", "an", "and", "announce", "another", "any", "anybody", "anyhow", "anymore", "anyone", "anything", "anyway", "anyways", "anywhere", "ao", "ap", "apart", "apparently", "appear", "appreciate", "appropriate", "approximately", "ar", "are", "aren", "arent", "aren't", "arise", "around", "as", "a's", "aside", "ask", "asking", "associated", "at", "au", "auth", "av", "available", "aw", "away", "awfully", "ax", "ay", "az", "b", "b1", "b2", "b3", "ba", "back", "bc", "bd", "be", "became", "because", "become", "becomes", "becoming", "been", "before", "beforehand", "begin", "beginning", "beginnings", "begins", "behind", "being", "believe", "below", "beside", "besides", "best", "better", "between", "beyond", "bi", "bill", "biol", "bj", "bk", "bl", "bn", "both", "bottom", "bp", "br", "brief", "briefly", "bs", "bt", "bu", "but", "bx", "by", "c", "c1", "c2", "c3", "ca", "call", "came", "can", "cannot", "cant", "can't", "cause", "causes", "cc", "cd", "ce", "certain", "certainly", "cf", "cg", "ch", "changes", "ci", "cit", "cj", "cl", "clearly", "cm", "c'mon", "cn", "co", "com", "come", "comes", "con", "concerning", "consequently", "consider", "considering", "contain", "containing", "contains", "corresponding", "could", "couldn", "couldnt", "couldn't", "course", "cp", "cq", "cr", "cry", "cs", "c's", "ct", "cu", "currently", "cv", "cx", "cy", "cz", "d", "d2", "da", "date", "dc", "dd", "de", "definitely", "describe", "described", "despite", "detail", "df", "di", "did", "didn", "didn't", "different", "dj", "dk", "dl", "do", "does", "doesn", "doesn't", "doing", "don", "done", "don't", "down", "downwards", "dp", "dr", "ds", "dt", "du", "due", "during", "dx", "dy", "e", "e2", "e3", "ea", "each", "ec", "ed", "edu", "ee", "ef", "effect", "eg", "ei", "eight", "eighty", "either", "ej", "el", "eleven", "else", "elsewhere", "em", "empty", "en", "end", "ending", "enough", "entirely", "eo", "ep", "eq", "er", "es", "especially", "est", "et", "et-al", "etc", "eu", "ev", "even", "ever", "every", "everybody", "everyone", "everything", "everywhere", "ex", "exactly", "example", "except", "ey", "f", "f2", "fa", "far", "fc", "few", "ff", "fi", "fifteen", "fifth", "fify", "fill", "find", "fire", "first", "five", "fix", "fj", "fl", "fn", "fo", "followed", "following", "follows", "for", "former", "formerly", "forth", "forty", "found", "four", "fr", "from", "front", "fs", "ft", "fu", "full", "further", "furthermore", "fy", "g", "ga", "gave", "ge", "get", "gets", "getting", "gi", "give", "given", "gives", "giving", "gj", "gl", "go", "goes", "going", "gone", "got", "gotten", "gr", "greetings", "gs", "gy", "h", "h2", "h3", "had", "hadn", "hadn't", "happens", "hardly", "has", "hasn", "hasnt", "hasn't", "have", "haven", "haven't", "having", "he", "hed", "he'd", "he'll", "hello", "help", "hence", "her", "here", "hereafter", "hereby", "herein", "heres", "here's", "hereupon", "hers", "herself", "hes", "he's", "hh", "hi", "hid", "him", "himself", "his", "hither", "hj", "ho", "home", "hopefully", "how", "howbeit", "however", "how's", "hr", "hs", "http", "hu", "hundred", "hy", "i", "i2", "i3", "i4", "i6", "i7", "i8", "ia", "ib", "ibid", "ic", "id", "i'd", "ie", "if", "ig", "ignored", "ih", "ii", "ij", "il", "i'll", "im", "i'm", "immediate", "immediately", "importance", "important", "in", "inasmuch", "inc", "indeed", "index", "indicate", "indicated", "indicates", "information", "inner", "insofar", "instead", "interest", "into", "invention", "inward", "io", "ip", "iq", "ir", "is", "isn", "isn't", "it", "itd", "it'd", "it'll", "its", "it's", "itself", "iv", "i've", "ix", "iy", "iz", "j", "jj", "jr", "js", "jt", "ju", "just", "k", "ke", "keep", "keeps", "kept", "kg", "kj", "km", "know", "known", "knows", "ko", "l", "l2", "la", "largely", "last", "lately", "later", "latter", "latterly", "lb", "lc", "le", "least", "les", "less", "lest", "let", "lets", "let's", "lf", "like", "liked", "likely", "line", "little", "lj", "ll", "ll", "ln", "lo", "look", "looking", "looks", "los", "lr", "ls", "lt", "ltd", "m", "m2", "ma", "made", "mainly", "make", "makes", "many", "may", "maybe", "me", "mean", "means", "meantime", "meanwhile", "merely", "mg", "might", "mightn", "mightn't", "mill", "million", "mine", "miss", "ml", "mn", "mo", "more", "moreover", "most", "mostly", "move", "mr", "mrs", "ms", "mt", "mu", "much", "mug", "must", "mustn", "mustn't", "my", "myself", "n", "n2", "na", "name", "namely", "nay", "nc", "nd", "ne", "near", "nearly", "necessarily", "necessary", "need", "needn", "needn't", "needs", "neither", "never", "nevertheless", "new", "next", "ng", "ni", "nine", "ninety", "nj", "nl", "nn", "no", "nobody", "non", "none", "nonetheless", "noone", "nor", "normally", "nos", "not", "noted", "nothing", "novel", "now", "nowhere", "nr", "ns", "nt", "ny", "o", "oa", "ob", "obtain", "obtained", "obviously", "oc", "od", "of", "off", "often", "og", "oh", "oi", "oj", "ok", "okay", "ol", "old", "om", "omitted", "on", "once", "one", "ones", "only", "onto", "oo", "op", "oq", "or", "ord", "os", "ot", "other", "others", "otherwise", "ou", "ought", "our", "ours", "ourselves", "out", "outside", "over", "overall", "ow", "owing", "own", "ox", "oz", "p", "p1", "p2", "p3", "page", "pagecount", "pages", "par", "part", "particular", "particularly", "pas", "past", "pc", "pd", "pe", "per", "perhaps", "pf", "ph", "pi", "pj", "pk", "pl", "placed", "please", "plus", "pm", "pn", "po", "poorly", "possible", "possibly", "potentially", "pp", "pq", "pr", "predominantly", "present", "presumably", "previously", "primarily", "probably", "promptly", "proud", "provides", "ps", "pt", "pu", "put", "py", "q", "qj", "qu", "que", "quickly", "quite", "qv", "r", "r2", "ra", "ran", "rather", "rc", "rd", "re", "readily", "really", "reasonably", "recent", "recently", "ref", "refs", "regarding", "regardless", "regards", "related", "relatively", "research", "research-articl", "respectively", "resulted", "resulting", "results", "rf", "rh", "ri", "right", "rj", "rl", "rm", "rn", "ro", "rq", "rr", "rs", "rt", "ru", "run", "rv", "ry", "s", "s2", "sa", "said", "same", "saw", "say", "saying", "says", "sc", "sd", "se", "sec", "second", "secondly", "section", "see", "seeing", "seem", "seemed", "seeming", "seems", "seen", "self", "selves", "sensible", "sent", "serious", "seriously", "seven", "several", "sf", "shall", "shan", "shan't", "she", "shed", "she'd", "she'll", "shes", "she's", "should", "shouldn", "shouldn't", "should've", "show", "showed", "shown", "showns", "shows", "si", "side", "significant", "significantly", "similar", "similarly", "since", "sincere", "six", "sixty", "sj", "sl", "slightly", "sm", "sn", "so", "some", "somebody", "somehow", "someone", "somethan", "something", "sometime", "sometimes", "somewhat", "somewhere", "soon", "sorry", "sp", "specifically", "specified", "specify", "specifying", "sq", "sr", "ss", "st", "still", "stop", "strongly", "sub", "substantially", "successfully", "such", "sufficiently", "suggest", "sup", "sure", "sy", "system", "sz", "t", "t1", "t2", "t3", "take", "taken", "taking", "tb", "tc", "td", "te", "tell", "ten", "tends", "tf", "th", "than", "thank", "thanks", "thanx", "that", "that'll", "thats", "that's", "that've", "the", "their", "theirs", "them", "themselves", "then", "thence", "there", "thereafter", "thereby", "thered", "therefore", "therein", "there'll", "thereof", "therere", "theres", "there's", "thereto", "thereupon", "there've", "these", "they", "theyd", "they'd", "they'll", "theyre", "they're", "they've", "thickv", "thin", "think", "third", "this", "thorough", "thoroughly", "those", "thou", "though", "thoughh", "thousand", "three", "throug", "through", "throughout", "thru", "thus", "ti", "til", "tip", "tj", "tl", "tm", "tn", "to", "together", "too", "took", "top", "toward", "towards", "tp", "tq", "tr", "tried", "tries", "truly", "try", "trying", "ts", "t's", "tt", "tv", "twelve", "twenty", "twice", "two", "tx", "u", "u201d", "ue", "ui", "uj", "uk", "um", "un", "under", "unfortunately", "unless", "unlike", "unlikely", "until", "unto", "uo", "up", "upon", "ups", "ur", "us", "use", "used", "useful", "usefully", "usefulness", "uses", "using", "usually", "ut", "v", "va", "value", "various", "vd", "ve", "ve", "very", "via", "viz", "vj", "vo", "vol", "vols", "volumtype", "vq", "vs", "vt", "vu", "w", "wa", "want", "wants", "was", "wasn", "wasnt", "wasn't", "way", "we", "wed", "we'd", "welcome", "well", "we'll", "well-b", "went", "were", "we're", "weren", "werent", "weren't", "we've", "what", "whatever", "what'll", "whats", "what's", "when", "whence", "whenever", "when's", "where", "whereafter", "whereas", "whereby", "wherein", "wheres", "where's", "whereupon", "wherever", "whether", "which", "while", "whim", "whither", "who", "whod", "whoever", "whole", "who'll", "whom", "whomever", "whos", "who's", "whose", "why", "why's", "wi", "widely", "will", "willing", "wish", "with", "within", "without", "wo", "won", "wonder", "wont", "won't", "words", "world", "would", "wouldn", "wouldnt", "wouldn't", "www", "x", "x1", "x2", "x3", "xf", "xi", "xj", "xk", "xl", "xn", "xo", "xs", "xt", "xv", "xx", "y", "y2", "yes", "yet", "yj", "yl", "you", "youd", "you'd", "you'll", "your", "youre", "you're", "yours", "yourself", "yourselves", "you've", "yr", "ys", "yt", "z", "zero", "zi", "zz"]
# print(stopwords)
# stopwords=[i.replace('"',"").strip() for i in stopwords]
# print(stopwords)

#clean the data
stop = stopwords
print(stop)
exclude = set(string.punctuation)
print(exclude)
lemma = WordNetLemmatizer()
def clean(text):
  stop_free = ' '.join([word for word in text.lower().split() if word not in stop])
  punc_free = ''.join(ch for ch in stop_free if ch not in exclude)
  # print(punc_free.split())
  # normalized = ' '.join([lemma.lemmatize(word) for word in punc_free.split()])
  return punc_free

df_topics['text_clean']=df_topics['Question'].apply(clean)
# print(df_topics['text_clean'])

list_questions = df_topics['text_clean'].values.tolist()
list_questions = list_questions[:67]
ques = []
index = 0
# print(list_questions)

!pip install bertopic
from bertopic import BERTopic

topicModel = BERTopic()
topics, probs = topicModel.fit_transform(list_questions)

topicModel.get_topic_info()

# This was a test, no longer needed
# topic0 = topicModel.get_topics()[-1]
# topic1 = topicModel.get_topics()[0]
# topic2 = topicModel.get_topics()[1]

# topics = []
# for index in range(3):
#   ind = index-1
#   topic = []
#   listTop = topicModel.get_topics()[ind]
#   for (x,y) in listTop:
#     topic.append(x)
#   topics.append(topic)

# print(topics)

from gensim.test.utils import common_corpus, common_dictionary
from gensim.models.coherencemodel import CoherenceModel

def calculate_coherence_score(topic_model, docs):
    # Preprocess documents
    cleaned_docs = topic_model._preprocess_text(docs)

    # Extract vectorizer and tokenizer from BERTopic
    vectorizer = topic_model.vectorizer_model
    tokenizer = vectorizer.build_tokenizer()

    # Extract features for Topic Coherence evaluation
    words = vectorizer.get_feature_names_out()
    # depending on the version and if you get an error use commented out code below:
    # words = vectorizer.get_feature_names()
    tokens = [tokenizer(doc) for doc in cleaned_docs]
    dictionary = corpora.Dictionary(tokens)
    corpus = [dictionary.doc2bow(token) for token in tokens]
    # Create topic words
    topic_words = [[dictionary.token2id[w] for w in words if w in dictionary.token2id] for _ in range(3)]

    # this creates a list of the token ids (in the format of integers) of the words in words that are also present in the
    # dictionary created from the preprocessed text. The topic_words list contains list of token ids for each
    # topic.
    coherence_model = CoherenceModel(topics=topic_words,
                                    texts=tokens,
                                    corpus=corpus,
                                    dictionary=dictionary,
                                    coherence='c_v')
    coherence = coherence_model.get_coherence()

    return coherence

df_google_sheet = pd.read_csv('https://docs.google.com/spreadsheets/d/1Ya2q9Ao1J4nQoifs5pWVoDyjZ4G970bAiFCW0D87nYA/export?format=csv', usecols=['Question', 'Practice Area','LLM #1: ChatGPT Generation','LLM #2: Claude Generation'])
df_google_sheet = df_google_sheet[:67]
gpt_doc = df_google_sheet['LLM #1: ChatGPT Generation'].values.tolist()
# gpt_doc = df_google_sheet['LLM #1: ChatGPT Generation'].apply(clean)
claude_doc = df_google_sheet['LLM #2: Claude Generation'].values.tolist()

print(df_google_sheet['Practice Area'][3])

# generate documents separate by practice area
civ_gpt = []
emp_gpt = []
ipr_gpt = []
cri_gpt = []
civ_claude = []
emp_claude = []
ipr_claude = []
cri_claude = []
for index, row in df_google_sheet.iterrows():
  pracArea = row['Practice Area']
  if pracArea == 'Civil':
    civ_gpt.append(row['LLM #1: ChatGPT Generation'])
    civ_claude.append(row['LLM #2: Claude Generation'])
  elif pracArea == 'Employment':
    emp_gpt.append(row['LLM #1: ChatGPT Generation'])
    emp_claude.append(row['LLM #2: Claude Generation'])
  elif pracArea == 'Intellectual Property':
    ipr_gpt.append(row['LLM #1: ChatGPT Generation'])
    ipr_claude.append(row['LLM #2: Claude Generation'])
  elif pracArea == 'Criminal':
    cri_gpt.append(row['LLM #1: ChatGPT Generation'])
    cri_claude.append(row['LLM #2: Claude Generation'])

print(civ_gpt)
print(civ_claude)
print(emp_gpt)
print(emp_claude)
print(ipr_gpt)
print(ipr_claude)
print(cri_gpt)
print(cri_claude)

# gpt scores
print("GPT Topic Coherence")
print("GPT Total",calculate_coherence_score(topicModel, gpt_doc))
print("GPT Civil",calculate_coherence_score(topicModel, civ_gpt))
print("GPT Employment",calculate_coherence_score(topicModel, emp_gpt))
print("GPT Intellectual Property",calculate_coherence_score(topicModel, ipr_gpt))
print("GPT Criminal",calculate_coherence_score(topicModel, cri_gpt))

# claude scores
print("Claude Topic Coherence")
print("Claude Total",calculate_coherence_score(topicModel, claude_doc))
print("Claude Civil",calculate_coherence_score(topicModel, civ_claude))
print("Claude Employment",calculate_coherence_score(topicModel, emp_claude))
print("Claude Intellectual Property",calculate_coherence_score(topicModel, ipr_claude))
print("Claude Criminal",calculate_coherence_score(topicModel, cri_claude))

print(calculate_coherence_score(topicModel, list_questions))

"""# **Generate Scores For Dataset**"""

from google.colab import drive
drive.mount("/content/drive")

import pandas as pd
from time import sleep
import gc

def get_score(df, func, score_name):
  questions, gpt, claude = [], [], []
  for index, row in df.iterrows():
    if index < 37 or index > 51:
      continue
    if len(row['Question']) == 0:
      break
    questions.append(row['Question'])
    print(row['Question'][:30])
    for id, llm_ans in enumerate([row['LLM #1: ChatGPT Generation'], row['LLM #2: Claude Generation']]):
      if type(llm_ans) != str or len(llm_ans) == 0:
        if id == 0:
          gpt.append(0.0)
        else:
          claude.append(0.0)
        continue
      scores = []
      for lawyer_ans in [row['Response from Lawyer #1'], row['Response from Lawyer #2 (if available)'], row['Response from Lawyer #3 (if available)']]:
        if type(lawyer_ans) != str:
          continue
        if score_name != 'BERT':
          gc.collect()
          sleep(10)
        s = func(lawyer_ans, llm_ans)
        print('LLM id:', id, s)
        scores.append(s)
      if id == 0:
        gpt.append(np.mean(scores))
      else:
        claude.append(np.mean(scores))
    print('****************************************')
  # return questions, gpt, claude
  df_score = pd.DataFrame({'Questions': questions, 'GPT': gpt, 'Claude': claude})
  display(df_score)
  df_score.to_csv('/content/drive/My Drive/Colab Notebooks/CS263/Project/Scores'+score_name+'.csv', encoding='utf-8')
  return df_score

df_google_sheet = pd.read_csv('https://docs.google.com/spreadsheets/d/1Ya2q9Ao1J4nQoifs5pWVoDyjZ4G970bAiFCW0D87nYA/export?format=csv')
df_bert_score = get_score(df_google_sheet, get_BERT_score, 'BERT')

df_google_sheet = pd.read_csv('https://docs.google.com/spreadsheets/d/1Ya2q9Ao1J4nQoifs5pWVoDyjZ4G970bAiFCW0D87nYA/export?format=csv')
df_bleurt_score = get_score(df_google_sheet, get_BLEURT_score, 'BLEURT')

"""=================================================================================================================

# **GPTScore**
Ref: https://arxiv.org/abs/2302.04166

https://github.com/jinlanfu/GPTScore
"""

!git clone https://github.com/jinlanfu/GPTScore.git
!pip install mosestokenizer
!pip3 install openai

# Commented out IPython magic to ensure Python compatibility.
!pwd
# %cd GPTScore

!python score_d2t.py \
--dataname "BAGEL" \
--use_demo False \
--use_ist True \
--opt350m_score True \
--out_dir_name "optScore_based" \
--aspect 'quality'

execfile('opt_score.py')
opt_scorer = OPTScorer(checkpoint="facebook/opt-350m")
text1 = "It depends on what was shared. The reason you needed time off is not inherently confidential unless the reason relates to your medical condition or the medical condition of a close relative. Therefore more needs to be known about the information that was shared before someone could tell you if it was legally considered confidential. Something is not confidential just because you want it to be."
text2 = "I am not a lawyer, but I can provide some general information on this topic. The legality of your manager sharing confidential information about your abortion with your fellow employees depends on the specific laws and regulations in your jurisdiction, as well as the policies of your workplace regarding privacy and confidentiality. In many countries, personal medical information is considered private and protected by laws such as the Health Insurance Portability and Accountability Act (HIPAA) in the United States. Disclosing such information without your consent could potentially be a violation of privacy laws. However, there may be exceptions or specific circumstances where sharing the information could be legally permitted. For example, if there is a legitimate business need to disclose the information or if it is required by law. It's important to consult with a legal professional or seek advice from your local labor authority to understand the specific legal protections and regulations that apply to your situation. Additionally, it's worth noting that workplace policies and codes of conduct often emphasize the importance of maintaining confidentiality and respecting employees' privacy. Violating these policies could have consequences for the manager and potentially lead to legal issues for the employer. If you have concerns about your manager sharing your confidential information, you may want to consider discussing your concerns with HR or a trusted representative within your organization to seek guidance and understand your rights in your specific workplace and jurisdiction."
opt_scorer.score([text1], [text2], prompt_text=" In other words , ", batch_size=1)

execfile('gpt3_score.py')
text1 = "It depends on what was shared. The reason you needed time off is not inherently confidential unless the reason relates to your medical condition or the medical condition of a close relative. Therefore more needs to be known about the information that was shared before someone could tell you if it was legally considered confidential. Something is not confidential just because you want it to be."
text2 = "I am not a lawyer, but I can provide some general information on this topic. The legality of your manager sharing confidential information about your abortion with your fellow employees depends on the specific laws and regulations in your jurisdiction, as well as the policies of your workplace regarding privacy and confidentiality. In many countries, personal medical information is considered private and protected by laws such as the Health Insurance Portability and Accountability Act (HIPAA) in the United States. Disclosing such information without your consent could potentially be a violation of privacy laws. However, there may be exceptions or specific circumstances where sharing the information could be legally permitted. For example, if there is a legitimate business need to disclose the information or if it is required by law. It's important to consult with a legal professional or seek advice from your local labor authority to understand the specific legal protections and regulations that apply to your situation. Additionally, it's worth noting that workplace policies and codes of conduct often emphasize the importance of maintaining confidentiality and respecting employees' privacy. Violating these policies could have consequences for the manager and potentially lead to legal issues for the employer. If you have concerns about your manager sharing your confidential information, you may want to consider discussing your concerns with HR or a trusted representative within your organization to seek guidance and understand your rights in your specific workplace and jurisdiction."
gpt3score(text1, text2, 'ada')

import argparse

execfile('score.py')
from argparse import Namespace

args = Namespace(device='cuda:0', aspect='quality', opt350m_score=True)
scorer = Scorer(args)
scorer.score(['gpt3_score'])
scorer.save_data("/content/drive/My Drive/Colab Notebooks/CS263/Project/Test.pkl")

# !python GPTScore/gpt_inference.py
# from GPTScore import gpt_inference
# !python gpt3_score.py
# from GPTScore import gpt3_score

execfile('gpt3_score.py')
score = gpt3score("I go to school by bus", "I go to school by bus as well")
print(score)

"""================================================================================================================="""

import pickle

with open('/content/drive/My Drive/Colab Notebooks/CS263/Project/gptscore_demo_bagel/data.pkl', 'rb') as f:
    data = pickle.load(f)
    print(data)

import pandas as pd

# Load the Excel file
file_path = 'q&a.xlsx'
df = pd.read_excel(file_path)

# Define the askGPT function (you need to implement this function)
def askGPT(prompt):
    # This is a placeholder for the actual implementation of the askGPT function
    # You should replace this with the actual API call to the LLM
    return {
        "pairwise_comparison_score_chatgpt": 0,
        "single_answer_grading_score_chatgpt": 0,
        "reference_guided_grading_score_chatgpt": 0,
        "pairwise_comparison_score_claude": 0,
        "single_answer_grading_score_claude": 0,
        "reference_guided_grading_score_claude": 0,
    }

# Prepare an empty list to store results
results = []

# Iterate over each row in the DataFrame
for index, row in df.iterrows():
    response_lawyer_1 = row['Response from Lawyer #1']
    response_lawyer_2 = row['Response from Lawyer #2 (if available)'] if 'Response from Lawyer #2 (if available)' in row else None
    response_lawyer_3 = row['Response from Lawyer #3 (if available)'] if 'Response from Lawyer #3 (if available)' in row else None
    llm1_chatgpt_generation = row['LLM #1: ChatGPT Generation']
    llm2_claude_generation = row['LLM #2: Claude Generation']

    # Generate the prompt
    prompt = f"""
    Evaluate the following responses using LLM-assisted scoring:
    Response from Lawyer #1: {response_lawyer_1}
    Response from Lawyer #2: {response_lawyer_2 if response_lawyer_2 else "N/A"}
    Response from Lawyer #3: {response_lawyer_3 if response_lawyer_3 else "N/A"}

    LLM #1: ChatGPT Generation: {llm1_chatgpt_generation}
    LLM #2: Claude Generation: {llm2_claude_generation}

    Please provide the following scores for each LLM generation:
    - Pairwise comparison
    - Single answer grading
    - Reference-guided grading

    Provide scores for both LLM #1: ChatGPT and LLM #2: Claude, and structure your response as:
    - Pairwise comparison score for ChatGPT
    - Single answer grading score for ChatGPT
    - Reference-guided grading score for ChatGPT
    - Pairwise comparison score for Claude
    - Single answer grading score for Claude
    - Reference-guided grading score for Claude
    """

    # Call the askGPT function
    response = ask_chatgpt(prompt)
    scores = extract_response_score(response)

    results.append({
        "Practice Area": row['Practice Area'],
        "Question": row['Question'],
        "Response from Lawyer #1": response_lawyer_1,
        "Response from Lawyer #2": response_lawyer_2,
        "Response from Lawyer #3": response_lawyer_3,
        "LLM #1: ChatGPT Generation": llm1_chatgpt_generation,
        "LLM #2: Claude Generation": llm2_claude_generation,
        "Pairwise Comparison Score (ChatGPT)": scores["pairwise_comparison_score_chatgpt"],
        "Single Answer Grading Score (ChatGPT)": scores["single_answer_grading_score_chatgpt"],
        "Reference-guided Grading Score (ChatGPT)": scores["reference_guided_grading_score_chatgpt"],
        "Pairwise Comparison Score (Claude)": scores["pairwise_comparison_score_claude"],
        "Single Answer Grading Score (Claude)": scores["single_answer_grading_score_claude"],
        "Reference-guided Grading Score (Claude)": scores["reference_guided_grading_score_claude"],
    })

# Convert the results to a DataFrame
results_df = pd.DataFrame(results)

# Save the results to a new Excel file
output_file_path = 'LLM_Evaluation_Results.xlsx'
results_df.to_excel(output_file_path, index=False)

print("Evaluation complete. Results saved to", output_file_path)